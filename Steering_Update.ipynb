{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "floyd run --gpu --mode jupyter --data itimmis/datasets/steering_new:steering_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For deep learning\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "\n",
    "# Synthetic data generation (image augmentation)\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# For image manipulation\n",
    "from sklearn.feature_extraction import image\n",
    "import scipy.misc\n",
    "\n",
    "# For math\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import math\n",
    "\n",
    "# For file manipulation\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "# For graphing\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "# For reading images\n",
    "from matplotlib.image import imread\n",
    "\n",
    "# For data manipulation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_training_history(history1, history2):\n",
    "\n",
    "    save_data = (history1, history2)\n",
    "    pickle_out = open('history.pickle', 'wb')\n",
    "    pickle.dump(save_data, pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_history():\n",
    "\n",
    "    pickle_in = open('history.pickle', 'rb')        \n",
    "    (history1, history2) = pickle.load(pickle_in)\n",
    "    \n",
    "    return (history1, history2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_OLD(data_dict, batch_size):\n",
    "    \n",
    "    # This function generates each mini-batch one at a time\n",
    "    \n",
    "    # Create empty arrays to contain batch of features and labels\n",
    "    batch_features = np.zeros((batch_size, data_dict[\"x_train\"][1].shape[0], data_dict[\"x_train\"][1].shape[1], data_dict[\"x_train\"][1].shape[2]))\n",
    "    batch_labels = np.zeros((batch_size, 1))\n",
    "   \n",
    "    while True:\n",
    "        for i in range(batch_size):\n",
    "            # choose random index in features\n",
    "            index = random.choice(len(data_dict[\"x_train\"]),1)\n",
    "            batch_features[i] = data_dict[\"x_train\"][index]\n",
    "            batch_labels[i] = data_dict[\"y_train\"][index]\n",
    "        yield batch_features, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(data_dict, batch_size):\n",
    "    \n",
    "    # This function generates each mini-batch one at a time\n",
    "    \n",
    "    # Create empty arrays to contain batch of features and labels\n",
    "    batch_features = np.zeros((batch_size, 640, 480, 3))\n",
    "    batch_labels = np.zeros((batch_size, 1))\n",
    "   \n",
    "    while True:\n",
    "        for i in range(0, batch_size, 2):\n",
    "            # choose random index in features\n",
    "            index = random.choice(len(data_dict[\"x_train\"]),1)\n",
    "            filename = data_dict[\"x_train\"][index]\n",
    "            \n",
    "            img = imread(filename)            \n",
    "            radian = filename.split('_')[-1].replace('.jpg','') # Remove the idx from filepath\n",
    "\n",
    "            # Image\n",
    "            batch_features[i] = img\n",
    "            batch_labels[i] = radians_to_degrees(float(radian))\n",
    "\n",
    "            # Image Flipped\n",
    "            batch_features[i + 1] = np.fliplr(img)\n",
    "            batch_labels[i + 1] = radians_to_degrees(float(radian)*-1)\n",
    "\n",
    "        batch_features /= 255.\n",
    "        yield batch_features, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def radians_to_degrees(x):\n",
    "    deg = math.degrees(x + math.pi)\n",
    "    return deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degrees_to_radians(x):\n",
    "    rad = math.radians(x) - math.pi\n",
    "    return rad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_OLD():\n",
    "    \n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    cloud = True\n",
    "    \n",
    "    folder_names = [\"campus_run1\", \"campus_run2\", \"run1\", \"run2\", \"run3\", \"second_spot1\", \"second_spot2\"]\n",
    "    \n",
    "    if cloud == True:\n",
    "        path = \"/steering_new/\"\n",
    "    else: \n",
    "        path = \"/Users/itimmis/Desktop/ACTor/Data/SteeringNew/\"\n",
    "    \n",
    "    # Loop folders\n",
    "    for folder in folder_names:\n",
    "        \n",
    "        # open folder\n",
    "        os.chdir(path + folder)\n",
    "        \n",
    "        # Loop files\n",
    "        for file in glob.glob(\"*.jpg\"):\n",
    "\n",
    "            # Load image\n",
    "            im = imread(file)\n",
    "            images.append(im)\n",
    "            images.append(np.fliplr(im))\n",
    "\n",
    "            # Extract label\n",
    "            value = file.split('_')[-1].replace('.jpg','') # Remove the idx from filepath\n",
    "            labels.append(radians_to_degrees(float(value)))\n",
    "            labels.append(radians_to_degrees(float(value)*-1))\n",
    "        \n",
    "    # Convert to numpy arrays\n",
    "    x = np.array(images)\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    # Extract Test Set\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\n",
    "\n",
    "    # Extract Validation Set\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=1)\n",
    "    \n",
    "    # Normalize images\n",
    "    x_train = x_train / 255.\n",
    "    x_val = x_val / 255.\n",
    "    x_test = x_test / 255.\n",
    "    \n",
    "    # Create data dictionary to contain train,val,test sets\n",
    "    data_dict = {\n",
    "        \"x_train\":x_train,\n",
    "        \"y_train\":y_train,\n",
    "        \"x_val\":x_val,\n",
    "        \"y_val\":y_val,\n",
    "        \"x_test\":x_test,\n",
    "        \"y_test\":y_test\n",
    "    }\n",
    "    \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    \n",
    "    folder_names = [\"campus_run1\", \"campus_run2\", \"run1\", \"run2\", \"run3\", \"second_spot1\", \"second_spot2\"]\n",
    "    \n",
    "    path = \"/steering_new/\"\n",
    "    \n",
    "    # Loop folders\n",
    "    for folder in folder_names:\n",
    "        \n",
    "        # open folder\n",
    "        os.chdir(path + folder)\n",
    "        \n",
    "        # Loop files\n",
    "        for file in sorted(glob.glob(\"*.jpg\")):\n",
    "\n",
    "            # Load image\n",
    "            x_train.append(file)\n",
    "\n",
    "    count = len(x_train) // 5 # 20%\n",
    "    \n",
    "    # Extract val set\n",
    "    x_val = x_train[-count:]\n",
    "    x_train = x_train[:-count]\n",
    "    \n",
    "    # Extract test set\n",
    "    x_test = x_train[-count:]\n",
    "    x_train = x_train[:count]\n",
    "    \n",
    "    # Load val set\n",
    "    y_val = []\n",
    "    for file in x_val:\n",
    "\n",
    "        # Load image\n",
    "        x_val.append(imread(file))\n",
    "\n",
    "        # Extract label\n",
    "        value = file.split('_')[-1].replace('.jpg','') # Remove the idx from filepath\n",
    "        y_val.append(radians_to_degrees(float(value)))\n",
    "        \n",
    "    # Loat test set\n",
    "    y_test = []\n",
    "    for file in x_test:\n",
    "\n",
    "        # Load image\n",
    "        x_test.append(imread(file))\n",
    "\n",
    "        # Extract label\n",
    "        value = file.split('_')[-1].replace('.jpg','') # Remove the idx from filepath\n",
    "        y_test.append(radians_to_degrees(float(value)))\n",
    "    \n",
    "    # Create data dictionary to contain train,val,test sets\n",
    "    data_dict = {\n",
    "        \"x_train\":x_train,\n",
    "        \"y_train\":y_train,\n",
    "        \"x_val\":x_val,\n",
    "        \"y_val\":y_val,\n",
    "        \"x_test\":x_test,\n",
    "        \"y_test\":y_test\n",
    "    }\n",
    "    \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(data_dict, hyperparameters):\n",
    "    \n",
    "    base_model = InceptionV3(weights=\"imagenet\", include_top=False, input_shape=data_dict[\"x_train\"][1].shape)\n",
    "    \n",
    "    # add a global spatial average pooling layer\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # add a fully-connected layer\n",
    "    x = Dense(hyperparameters[\"fc_size\"], activation=hyperparameters[\"fc_activation\"])(x)\n",
    "    \n",
    "    # add a logistic layer\n",
    "    predictions = Dense(1, kernel_initializer='normal')(x)\n",
    "    #Dense(num_classes, activation=output_activation)(x)\n",
    "    \n",
    "    # train this model\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    # first: train only the top layers (which were randomly initialized)\n",
    "    # i.e. freeze all convolutional InceptionV3 layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # compile the model (should be done *after* setting layers to non-trainable)\n",
    "    model.compile(optimizer='adam', loss=hyperparameters[\"loss\"], metrics=hyperparameters[\"metrics\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def main():\n",
    "\n",
    "# Hyperparameters\n",
    "hyperparameters = {\n",
    "    \"batchsize\" : 32,\n",
    "    \"fc_size\" : 1024,\n",
    "    \"fc_activation\" : 'relu',\n",
    "    \"epoch_finetune\" : 50,\n",
    "    \"epoch_transfer\" : 50,\n",
    "    \"loss\" : \"mean_squared_error\",\n",
    "    \"metrics\" : None,#[\"accuracy\"]\n",
    "    \"monitor\" : 'val_loss'\n",
    "}\n",
    "\n",
    "# # Load steering images\n",
    "data_dict = load_data()\n",
    "\n",
    "# # Create model\n",
    "model = init_model(data_dict, hyperparameters)\n",
    "\n",
    "m = data_dict[\"x_train\"].shape[0]\n",
    "\n",
    "# Train model\n",
    "#train_model(data_dict, hyperparameters)\n",
    "\n",
    "#    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(data_dict, hyperparameters):\n",
    "    \n",
    "top_weights_path = \"/output/top_weights.h5\"\n",
    "\n",
    "# Early Stopping and Model Checkpoint callbacks\n",
    "callbacks_list = [\n",
    "    ModelCheckpoint(top_weights_path, monitor=hyperparameters[\"monitor\"], verbose=1, save_best_only=True),\n",
    "    EarlyStopping(monitor=hyperparameters[\"monitor\"], patience=5, verbose=0)\n",
    "]\n",
    "\n",
    "# train the model on the new data for a few epochs\n",
    "history1 = model.fit_generator(generator(data_dict, hyperparameters[\"batchsize\"]), steps_per_epoch= m / hyperparameters[\"batchsize\"], \n",
    "                    epochs=hyperparameters[\"epoch_finetune\"], callbacks=callbacks_list, validation_data=(data_dict[\"x_val\"], data_dict[\"y_val\"]))\n",
    "\n",
    "model.load_weights(top_weights_path)\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers[:249]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Early Stopping and Model Checkpoint callbacks\n",
    "final_weights_path = \"/output/final_weights.h5\"\n",
    "callbacks_list = [\n",
    "    ModelCheckpoint(final_weights_path, monitor=hyperparameters[\"monitor\"], verbose=1, save_best_only=True),\n",
    "    EarlyStopping(monitor=hyperparameters[\"monitor\"], patience=5, verbose=0)\n",
    "]\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "from keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss=hyperparameters[\"loss\"], metrics=hyperparameters[\"metrics\"])\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "history2 = model.fit_generator(generator(data_dict,hyperparameters[\"batchsize\"]), steps_per_epoch= m / hyperparameters[\"batchsize\"], \n",
    "                    epochs=hyperparameters[\"epoch_transfer\"], callbacks=callbacks_list, validation_data=(data_dict[\"x_val\"], data_dict[\"y_val\"]))\n",
    "\n",
    "    # Save the transfer learning and fine tuning history\n",
    "    #save_training_history(history1, history2)\n",
    "    \n",
    "#     return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_weights_path = \"/output/final_weights.h5\"\n",
    "\n",
    "model.load_weights(final_weights_path)\n",
    "\n",
    "preds = model.evaluate(data_dict[\"x_test\"], data_dict[\"y_test\"])\n",
    "\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history1.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history2.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
