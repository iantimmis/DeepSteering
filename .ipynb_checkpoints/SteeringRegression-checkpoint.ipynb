{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "floyd run --gpu --mode jupyter --data itimmis/datasets/steering_new:steering_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For deep learning\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "\n",
    "# Synthetic data generation (image augmentation)\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# For image manipulation\n",
    "from sklearn.feature_extraction import image\n",
    "import scipy.misc\n",
    "\n",
    "# For math\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import math\n",
    "\n",
    "# For file manipulation\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "# For graphing\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "# For reading images\n",
    "from matplotlib.image import imread\n",
    "\n",
    "# For data manipulation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_training_history(history1, history2):\n",
    "\n",
    "    save_data = (history1, history2)\n",
    "    pickle_out = open('history.pickle', 'wb')\n",
    "    pickle.dump(save_data, pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_training_history():\n",
    "\n",
    "    pickle_in = open('history.pickle', 'rb')        \n",
    "    (history1, history2) = pickle.load(pickle_in)\n",
    "    \n",
    "    return (history1, history2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(data_dict, batch_size):\n",
    "    \n",
    "    # This function generates each mini-batch one at a time\n",
    "    \n",
    "    # Create empty arrays to contain batch of features and labels\n",
    "    batch_features = np.zeros((batch_size, data_dict[\"x_train\"][1].shape[0], data_dict[\"x_train\"][1].shape[1], data_dict[\"x_train\"][1].shape[2]))\n",
    "    batch_labels = np.zeros((batch_size, 1))\n",
    "   \n",
    "    while True:\n",
    "        for i in range(batch_size):\n",
    "            # choose random index in features\n",
    "            index = random.choice(len(data_dict[\"x_train\"]),1)\n",
    "            batch_features[i] = data_dict[\"x_train\"][index]\n",
    "            batch_labels[i] = data_dict[\"y_train\"][index]\n",
    "        yield batch_features, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def radians_to_degrees(x):\n",
    "    deg = math.degrees(x + math.pi)\n",
    "    return deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def degrees_to_radians(x):\n",
    "    rad = math.radians(x) - math.pi\n",
    "    return rad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    cloud = True\n",
    "    \n",
    "    folder_names = [\"campus_run1\", \"campus_run2\", \"run1\", \"run2\", \"run3\", \"second_spot1\", \"second_spot2\"]\n",
    "    \n",
    "    if cloud == True:\n",
    "        path = \"/steering_new/\"\n",
    "    else: \n",
    "        path = \"/Users/itimmis/Desktop/ACTor/Data/SteeringNew/\"\n",
    "    \n",
    "    # Loop folders\n",
    "    for folder in folder_names:\n",
    "        \n",
    "        # open folder\n",
    "        os.chdir(path + folder)\n",
    "        \n",
    "        # Loop files\n",
    "        for file in glob.glob(\"*.jpg\"):\n",
    "\n",
    "            # Load image\n",
    "            images.append(imread(file))\n",
    "\n",
    "            # Extract label\n",
    "            value = file.split('_')[-1].replace('.jpg','') # Remove the idx from filepath\n",
    "            labels.append(radians_to_degrees(float(value)))\n",
    "        \n",
    "    # Convert to numpy arrays\n",
    "    x = np.array(images)\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    # Extract Test Set\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\n",
    "\n",
    "    # Extract Validation Set\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=1)\n",
    "    \n",
    "    # Normalize images\n",
    "    x_train = x_train / 255.\n",
    "    x_val = x_val / 255.\n",
    "    x_test = x_test / 255.\n",
    "    \n",
    "    # Create data dictionary to contain train,val,test sets\n",
    "    data_dict = {\n",
    "        \"x_train\":x_train,\n",
    "        \"y_train\":y_train,\n",
    "        \"x_val\":x_val,\n",
    "        \"y_val\":y_val,\n",
    "        \"x_test\":x_test,\n",
    "        \"y_test\":y_test\n",
    "    }\n",
    "    \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dict = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(data_dict[\"x_train\"].shape)\n",
    "print(data_dict[\"y_train\"].shape)\n",
    "print(data_dict[\"x_val\"].shape)\n",
    "print(data_dict[\"y_val\"].shape)\n",
    "print(data_dict[\"x_test\"].shape)\n",
    "print(data_dict[\"y_test\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_model(data_dict, hyperparameters):\n",
    "    \n",
    "    base_model = InceptionV3(weights=\"imagenet\", include_top=False, input_shape=data_dict[\"x_train\"][1].shape)\n",
    "    \n",
    "    # add a global spatial average pooling layer\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # add a fully-connected layer\n",
    "    x = Dense(hyperparameters[\"fc_size\"], activation=hyperparameters[\"fc_activation\"])(x)\n",
    "    \n",
    "    # add a logistic layer\n",
    "    predictions = Dense(1, kernel_initializer='normal')(x)\n",
    "    #Dense(num_classes, activation=output_activation)(x)\n",
    "    \n",
    "    # train this model\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    # first: train only the top layers (which were randomly initialized)\n",
    "    # i.e. freeze all convolutional InceptionV3 layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # compile the model (should be done *after* setting layers to non-trainable)\n",
    "    model.compile(optimizer='adam', loss=hyperparameters[\"loss\"], metrics=hyperparameters[\"metrics\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def main():\n",
    "\n",
    "# Hyperparameters\n",
    "hyperparameters = {\n",
    "    \"batchsize\" : 32,\n",
    "    \"fc_size\" : 1024,\n",
    "    \"fc_activation\" : 'relu',\n",
    "    \"epoch_finetune\" : 30,\n",
    "    \"epoch_transfer\" : 30,\n",
    "    \"loss\" : \"mean_squared_error\",\n",
    "    \"metrics\" : None,#[\"accuracy\"]\n",
    "    \"monitor\" : 'val_loss'\n",
    "}\n",
    "\n",
    "# # Load steering images\n",
    "data_dict = load_data()\n",
    "\n",
    "# # Create model\n",
    "model = init_model(data_dict, hyperparameters)\n",
    "\n",
    "m = data_dict[\"x_train\"].shape[0]\n",
    "\n",
    "# Train model\n",
    "#train_model(data_dict, hyperparameters)\n",
    "\n",
    "#    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def train_model(data_dict, hyperparameters):\n",
    "    \n",
    "top_weights_path = \"top_weights.h5\"\n",
    "\n",
    "# Early Stopping and Model Checkpoint callbacks\n",
    "callbacks_list = [\n",
    "    ModelCheckpoint(top_weights_path, monitor=hyperparameters[\"monitor\"], verbose=1, save_best_only=True),\n",
    "    EarlyStopping(monitor=hyperparameters[\"monitor\"], patience=5, verbose=0)\n",
    "]\n",
    "\n",
    "# train the model on the new data for a few epochs\n",
    "history1 = model.fit_generator(generator(data_dict, hyperparameters[\"batchsize\"]), steps_per_epoch= m / hyperparameters[\"batchsize\"], \n",
    "                    epochs=hyperparameters[\"epoch_finetune\"], callbacks=callbacks_list, validation_data=(data_dict[\"x_val\"], data_dict[\"y_val\"]))\n",
    "\n",
    "# add the best weights from the train top model\n",
    "# at this point we have the pre-train weights of the base model and the trained weight of the new/added top model\n",
    "# we re-load model weights to ensure the best epoch is selected and not the last one.\n",
    "model.load_weights(top_weights_path)\n",
    "\n",
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "#     for i, layer in enumerate(base_model.layers):\n",
    "#         print(i, layer.name)\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers[:249]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Early Stopping and Model Checkpoint callbacks\n",
    "final_weights_path = \"final_weights.h5\"\n",
    "callbacks_list = [\n",
    "    ModelCheckpoint(final_weights_path, monitor=hyperparameters[\"monitor\"], verbose=1, save_best_only=True),\n",
    "    EarlyStopping(monitor=hyperparameters[\"monitor\"], patience=5, verbose=0)\n",
    "]\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "from keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss=hyperparameters[\"loss\"], metrics=hyperparameters[\"metrics\"])\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "history2 = model.fit_generator(generator(data_dict,hyperparameters[\"batchsize\"]), steps_per_epoch= m / hyperparameters[\"batchsize\"], \n",
    "                    epochs=hyperparameters[\"epoch_transfer\"], callbacks=callbacks_list, validation_data=(data_dict[\"x_val\"], data_dict[\"y_val\"]))\n",
    "\n",
    "    # Save the transfer learning and fine tuning history\n",
    "    #save_training_history(history1, history2)\n",
    "    \n",
    "#     return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_weights_path = \"final_weights.h5\"\n",
    "\n",
    "model.load_weights(final_weights_path)\n",
    "\n",
    "preds = model.evaluate(data_dict[\"x_test\"], data_dict[\"y_test\"])\n",
    "\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history1.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history2.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
